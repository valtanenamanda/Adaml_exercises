{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d74b3af",
   "metadata": {},
   "source": [
    "Credits to this tutorial: https://www.geeksforgeeks.org/deep-learning/implementing-recurrent-neural-networks-in-pytorch/\n",
    "\n",
    "The dataset used can be found at: https://www.kaggle.com/datasets/tanishqdublish/text-classification-documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a9a9c",
   "metadata": {},
   "source": [
    "The purpose of the task is to train the RNN model to classify text phrases. I used PyTorch library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0f18da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea430d3",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bca2c8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('df_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcabf4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2225 entries, 0 to 2224\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Text    2225 non-null   object\n",
      " 1   Label   2225 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 34.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Display data info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64610e6",
   "metadata": {},
   "source": [
    "The data has two columns, the first containing text phrases and the second containing the corresponding category (class/label)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121bbc04",
   "metadata": {},
   "source": [
    "During data preprocessing, missing values are checked for, text is converted to lowercase, and words are separated from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "736debc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text     0\n",
      "Label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Missing values count\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbc5be94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].str.lower().str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff89ae16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[budget, to, set, scene, for, election, gordon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[army, chiefs, in, regiments, decision, milita...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[howard, denies, split, over, id, cards, micha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[observers, to, monitor, uk, election, ministe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[kilroy, names, election, seat, target, ex-cha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Label\n",
       "0  [budget, to, set, scene, for, election, gordon...      0\n",
       "1  [army, chiefs, in, regiments, decision, milita...      0\n",
       "2  [howard, denies, split, over, id, cards, micha...      0\n",
       "3  [observers, to, monitor, uk, election, ministe...      0\n",
       "4  [kilroy, names, election, seat, target, ex-cha...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c55d8b",
   "metadata": {},
   "source": [
    "Train-test split (random based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42b76123",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42357a16",
   "metadata": {},
   "source": [
    "Encoding the text into integer sequences by building a vocabulary and assigning each token a unique index. Neural networks require fixed-length inputs, so different maximum sequence lengths were tested. Despite an average sequence length of nearly 400 tokens, the best results were achieved using only the first 50 words, with shorter sequences padded with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8f4fd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4432\n",
      "384.04044943820224\n"
     ]
    }
   ],
   "source": [
    "vocab = {word for phrase in df[\"Text\"] for word in phrase}\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab, start=1)}\n",
    "\n",
    "max_length = df[\"Text\"].str.len().max()\n",
    "print(max_length)\n",
    "avg_len = df[\"Text\"].str.len().mean()\n",
    "print(avg_len)\n",
    "max_length = 50\n",
    "\n",
    "def encode_and_pad(text):\n",
    "    encoded = [word_to_idx[word] for word in text]\n",
    "    if len(encoded) >= max_length:\n",
    "        encoded = encoded[:max_length]\n",
    "    else:\n",
    "        encoded = encoded + [0] * (max_length - len(encoded))\n",
    "    return encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75c6f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Text'] = train_data['Text'].apply(encode_and_pad)\n",
    "test_data['Text'] = test_data['Text'].apply(encode_and_pad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61effff6",
   "metadata": {},
   "source": [
    "Create dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8007cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.texts = data['Text'].values\n",
    "        self.labels = data['Label'].values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(text, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "train_dataset = TextDataset(train_data)\n",
    "test_dataset = TextDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c8e41",
   "metadata": {},
   "source": [
    "Creating RNN model which has embedding, RNN and output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c40b039",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size):\n",
    "        super(TextRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "vocab_size = len(vocab) + 1\n",
    "embed_size = 60\n",
    "hidden_size = 60\n",
    "output_size = 5  # Number of classes \n",
    "model = TextRNN(vocab_size, embed_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bee2c7b",
   "metadata": {},
   "source": [
    "Model training using CrossEntropy loss function and Adam optimized with learning rate = 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cd775a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 1.6270\n",
      "Epoch [2/50], Loss: 1.5479\n",
      "Epoch [3/50], Loss: 1.4791\n",
      "Epoch [4/50], Loss: 1.3814\n",
      "Epoch [5/50], Loss: 1.2506\n",
      "Epoch [6/50], Loss: 1.0887\n",
      "Epoch [7/50], Loss: 0.8852\n",
      "Epoch [8/50], Loss: 0.7070\n",
      "Epoch [9/50], Loss: 0.5671\n",
      "Epoch [10/50], Loss: 0.4264\n",
      "Epoch [11/50], Loss: 0.2953\n",
      "Epoch [12/50], Loss: 0.2091\n",
      "Epoch [13/50], Loss: 0.1489\n",
      "Epoch [14/50], Loss: 0.1118\n",
      "Epoch [15/50], Loss: 0.1021\n",
      "Epoch [16/50], Loss: 0.1075\n",
      "Epoch [17/50], Loss: 0.0628\n",
      "Epoch [18/50], Loss: 0.0391\n",
      "Epoch [19/50], Loss: 0.0267\n",
      "Epoch [20/50], Loss: 0.0200\n",
      "Epoch [21/50], Loss: 0.0390\n",
      "Epoch [22/50], Loss: 0.0836\n",
      "Epoch [23/50], Loss: 0.0284\n",
      "Epoch [24/50], Loss: 0.0176\n",
      "Epoch [25/50], Loss: 0.0112\n",
      "Epoch [26/50], Loss: 0.0090\n",
      "Epoch [27/50], Loss: 0.0078\n",
      "Epoch [28/50], Loss: 0.0068\n",
      "Epoch [29/50], Loss: 0.0061\n",
      "Epoch [30/50], Loss: 0.0055\n",
      "Epoch [31/50], Loss: 0.0050\n",
      "Epoch [32/50], Loss: 0.0045\n",
      "Epoch [33/50], Loss: 0.0041\n",
      "Epoch [34/50], Loss: 0.0038\n",
      "Epoch [35/50], Loss: 0.0035\n",
      "Epoch [36/50], Loss: 0.0032\n",
      "Epoch [37/50], Loss: 0.0030\n",
      "Epoch [38/50], Loss: 0.0027\n",
      "Epoch [39/50], Loss: 0.0026\n",
      "Epoch [40/50], Loss: 0.0024\n",
      "Epoch [41/50], Loss: 0.0022\n",
      "Epoch [42/50], Loss: 0.0021\n",
      "Epoch [43/50], Loss: 0.0020\n",
      "Epoch [44/50], Loss: 0.0019\n",
      "Epoch [45/50], Loss: 0.0017\n",
      "Epoch [46/50], Loss: 0.0016\n",
      "Epoch [47/50], Loss: 0.0015\n",
      "Epoch [48/50], Loss: 0.0015\n",
      "Epoch [49/50], Loss: 0.0014\n",
      "Epoch [50/50], Loss: 0.0013\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for texts, labels in train_loader:\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss / len(train_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f586f120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 58.43%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        outputs = model(texts)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8d88b1",
   "metadata": {},
   "source": [
    "When using 50 epochs, the model achieves an accuracy of 58.43% on the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
